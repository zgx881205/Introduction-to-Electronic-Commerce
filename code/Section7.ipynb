{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "366186d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy.optimize._highs._highs_wrapper'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-25d91f055065>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#from imblearn.over_sampling import RandomOverSampler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m from .utils._tags import (\n\u001b[0;32m     19\u001b[0m     \u001b[0m_DEFAULT_TAGS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmurmurhash\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmurmurhash3_32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_joblib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\class_weight.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_deprecate_positional_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcontextlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msuppress\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_object_dtype_isnan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_version\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_config\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_get_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPositiveSpectrumWarning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlsqr\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msparse_lsqr\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMaskedArray\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_MaskedArray\u001b[0m  \u001b[1;31m# TODO: remove in 1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\stats\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m \"\"\"\n\u001b[1;32m--> 391\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmorestats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mspecial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmstats_basic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m from ._stats_mstats_common import (_find_repeats, linregress, theilslopes,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\stats\\distributions.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#       instead of `git blame -Lxxx,+x`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m from ._distn_infrastructure import (entropy, rv_discrete, rv_continuous,\n\u001b[0m\u001b[0;32m      9\u001b[0m                                     rv_frozen)\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# for root finding for continuous distribution ppf, and max likelihood estimation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# for functions of continuous distributions (e.g. moments, entropy, cdf)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_nnls\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnnls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_basinhopping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbasinhopping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_linprog\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlinprog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinprog_verbose_callback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_lsap\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlinear_sum_assignment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_differentialevolution\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdifferential_evolution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_linprog.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOptimizeResult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOptimizeWarning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mwarnings\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_linprog_highs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_linprog_highs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_linprog_ip\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_linprog_ip\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_linprog_simplex\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_linprog_simplex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_linprog_highs.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_check_unknown_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOptimizeWarning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mwarnings\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_highs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_highs_wrapper\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_highs_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m from ._highs._highs_constants import (\n\u001b[0;32m     22\u001b[0m     \u001b[0mCONST_I_INF\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'scipy.optimize._highs._highs_wrapper'"
     ]
    }
   ],
   "source": [
    "# -*- encoding:utf-8 -*-\n",
    "#from sklearn import datasets\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "import math\n",
    "#from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import csv\n",
    "import os\n",
    "import heapq\n",
    "import time\n",
    "import datetime\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "starttime=datetime.datetime.now()\n",
    "import argparse\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "Dir=os.path.abspath('')\n",
    "def Sigmoid(z):\n",
    "\tG_of_Z = float(1.0 / float((1.0 + np.exp(-1.0*z))))\n",
    "\treturn G_of_Z\n",
    "\n",
    "##The hypothesis is the linear combination of all the known factors x[i] and their current estimated coefficients theta[i]\n",
    "##This hypothesis will be used to calculate each instance of the Cost Function\n",
    "def Hypothesis(x,theta):\n",
    "    z = 0\n",
    "    for i in xrange(0,len(theta)-1):\n",
    "        z += x[i]*theta[i]\n",
    "    z=z+theta[-1]\n",
    "    return z\n",
    "def loadDataSet(datatype):\n",
    "    dataMat = []\n",
    "    labelMat = []\n",
    "    file='/Feature '+datatype+'.csv'\n",
    "    Testcsv_file = open(Dir + file, 'r')\n",
    "    Testcsv_reader = csv.reader(Testcsv_file)\n",
    "    FeatureList = []  # 会话的特征矩阵\n",
    "    SessionIDList = []\n",
    "    LabelList = []  # 会话的Label矩阵\n",
    "    for data in Testcsv_reader:\n",
    "        #print data\n",
    "        SessionID = int(data[0])\n",
    "        A1 = float(data[1])\n",
    "        A2 = float(data[2])\n",
    "        A3 = int(data[3])\n",
    "        A4 = int(data[4])\n",
    "        A5 = int(data[5])\n",
    "        A6 = float(data[6])\n",
    "        A7 = float(data[7])\n",
    "        A8 = float(data[8])\n",
    "        A9 = float(data[9])\n",
    "        A10 = float(data[10])\n",
    "        A11 = float(data[11])\n",
    "        A12 = float(data[12])\n",
    "        A13 = float(data[13])\n",
    "        A14 = float(data[14])\n",
    "        A15 = float(data[15])\n",
    "        A16 = int(data[16])\n",
    "        A17 = int(data[17])\n",
    "        A18 = int(data[18])\n",
    "        A19 = int(data[19].rstrip(']').lstrip('[').replace(',','').replace(' ',''))#转换为二进制\n",
    "        A20 = int(data[20].rstrip(']').lstrip('[').replace(',','').replace(' ',''))\n",
    "        A21 = int(data[21].rstrip(']').lstrip('[').replace(',','').replace(' ',''))\n",
    "        A22 = float(data[22])\n",
    "        label = int(data[23])\n",
    "        User=int(data[24])\n",
    "        #feature = [A1, A2, A3,A4, 1 / (1 + math.e ** (-random.uniform(0,2)*(0.2*label+0.5))), A6, A7, A8, A9, A10, User, A11, A12, A13, A14, A15, A16, A17, A18, A19, A20, A21,A22]\n",
    "        #feature = [A1, A2, A3, A4, A5, A9, A10, A11, A12, A13, A14, A15, A16, A17, A18, A19, A20, A21, A22]\n",
    "        #feature = [A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, User, A11, A12, A13, A14, A15, A16, A17, A18, A19, A20, A21,A22]\n",
    "        feature = [A1, A2, A3,A4, A5, A6, A7, A8, A9, A10, User, A11, A12, A13, A14, A15, A16, A17, A18, A19, A20, A21,A22]\n",
    "        dataMat.append(feature)\n",
    "        #dataMat.append([1, line[1], line[2], line[3], line[4], line[5], line[6], line[7], line[8], line[9], line[10]])\n",
    "        # dataMat.append([1, float(lineArr[1]), float(lineArr[2]), float(lineArr[3])])\n",
    "        labelMat.append(label)\n",
    "    #Unordernum=len(labelMat)\n",
    "\n",
    "    #print 'Ordering Session:' + str(len(labelMat)-Unordernum)\n",
    "    dataMat = array(dataMat)\n",
    "    labelMat = array(labelMat)\n",
    "    #print dataMat\n",
    "    #print 'len'+str(len(dataMat[:,0]))\n",
    "    return dataMat, labelMat\n",
    "def LRClassifier(data):\n",
    "    print('\\n')\n",
    "    print(\"#############*******Co-EM LR **********###############\")\n",
    "    print ('data:' + str(data))\n",
    "    dataMat, labelMat = loadDataSet(data)\n",
    "    print('Original dataset shape {}'.format(Counter(labelMat)))\n",
    "    #plotBestFit(dataMat, labelMat, array(theta))\n",
    "    #theta = newtonMethod(dataMat, labelMat, 2)\n",
    "    #K-flod Cross Vlidation#\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataMat[:, :], labelMat, test_size=0.9, random_state=None) #初步划分测试集和训练集合\n",
    "    Plist=[]\n",
    "    Rlist=[]\n",
    "    Flist=[]\n",
    "    #for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    #print '\\n' + str(f) + '-flod:'\n",
    "    #f = f + 1\n",
    "    #X_train, X_test = X[train_index], X[test_index]  # 拆分训练集和测试集\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(dataMat[:,:], labelMat, test_size=0.3, random_state=None)\n",
    "    # 为了追求机器学习和最优化算法的最佳性能，我们将特征缩放\n",
    "    #欠采样\n",
    "    # rus = RandomUnderSampler(random_state=0)\n",
    "    # X_train, y_train = rus.fit_sample(X_train, y_train)\n",
    "    S=11  #对特征分割两个VIEW\n",
    "    E=23#对特征分割两个VIEW\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(X_train)  # 估算每个特征的平均值和标准差\n",
    "    X_train_std = sc.transform(X_train)\n",
    "    # 注意：这里我们要用同样的参数来标准化测试集，使得测试集和训练集之间有可比性\n",
    "    X_test_std = sc.transform(X_test)\n",
    "    Usertype=X_test[:,S-1]\n",
    "    #print \"UserTpe\",Usertype\n",
    "    FirstUser_std=X_test_std[np.argwhere(Usertype==0)[0][0]][S-1]  #用户类型规格化后数值\n",
    "    print (\"FirstUser_std:\",FirstUser_std)\n",
    "    Evervisit_std=X_test_std[np.argwhere(Usertype==1)[0][0]][S-1]\n",
    "    print ('Evervisit_std:',Evervisit_std)\n",
    "    Recent_std=X_test_std[np.argwhere(Usertype==2)[0][0]][S-1]\n",
    "    print (\"Recent_std:\",Recent_std)\n",
    "    X_combined_std = vstack((X_train_std, X_test_std))\n",
    "    y_combined = hstack((y_train, y_test))\n",
    "    # 对于训练集进行拆分，标记和未标记2部分数据：\n",
    "    # 为了追求机器学习和最优化算法的最佳性能，我们将特征规格化\n",
    "    # print\"Start SMOTE Resampling:\"\n",
    "    # sme = SMOTEENN(random_state=42) #neighbor\n",
    "    # X_train_std, y_train = sme.fit_sample( X_train_std, y_train)\n",
    "\n",
    "    # print \"Start Oversampling:\"\n",
    "    # ros = RandomOverSampler(random_state=0)\n",
    "    # X_train_std, y_train = ros.fit_sample(X_train_std,y_train)  #Over Sampler\n",
    "    # print('Oversampling dataset shape {}'.format(Counter(y_train)))\n",
    "    # print\"Start  SMOTE Resampling:\"\n",
    "    # sme = SMOTEENN(random_state=20) #neighbor\n",
    "    # X_train_std, y_train = sme.fit_sample(X_train_std,y_train)\n",
    "    # print('SMOTE dataset shape {}'.format(Counter(y_train)))\n",
    "    D_L=X_train_std\n",
    "    D_U=X_test_std\n",
    "    y_L=y_train\n",
    "    y_U=y_test\n",
    "    V1_D_Test = array(X_test_std[:, 0:S])\n",
    "    V2_D_Test = array(X_test_std[:, S:E])\n",
    "    print ('Scale of D_L data set' + str(Counter(y_L)))\n",
    "    print ('Scale of D_U data set' + str(Counter(y_U)))\n",
    "    V1_D_L = array(D_L[:, 0:S])\n",
    "    V2_D_L = array(D_L[:, S:E])\n",
    "    V1_D_U = array(D_U[:, 0:S])\n",
    "    V2_D_U = array(D_U[:, S:E])\n",
    "    print ('Scale of V1_DL:', np.shape(V1_D_L[:,0:S-1]))\n",
    "    print ('Scale of V2_DL:', np.shape(V2_D_L))\n",
    "    # 初始化LR训练计算#\n",
    "    lr_V2 = LogisticRegression(C=1e5, solver='sag',tol=1e-5,max_iter=5000,multi_class='ovr',class_weight={0: 0.01, 1: 0.01})  # LR参数设置final\n",
    "    #lr_V2= SGDClassifier(loss=\"log\", penalty=\"l2\",tol=1e-5,max_iter=5000,class_weight={0: 0.01, 1: 0.05})  #SGD梯度下降\n",
    "    # Co-traing 利用V2上的theta2\n",
    "    lr_V2.fit(V2_D_L, y_L)  # DL中的V2训练\n",
    "    # print \"lr_V2:\",lr_V2\n",
    "    print ('初始Lr_V2 Coef:' + str(lr_V2.coef_))\n",
    "    print ('初始Lr_V2 intercept_:' + str(lr_V2.intercept_))\n",
    "    print ('初始Lr_V2 n_iter_:' + str(lr_V2.n_iter_))\n",
    "    print (\"初始化的LR_V2的预测效果:\")\n",
    "    prediction_V2_Test = lr_V2.predict(V2_D_Test)\n",
    "    proba_V2_Test = lr_V2.predict_proba(V2_D_Test)\n",
    "    print(classification_report(y_test, prediction_V2_Test))\n",
    "    # for i in range(0,len(V2_D_Test)):\n",
    "    #     feature=V2_D_Test[i].reshape(1, -1)\n",
    "    #     label=y_test[i]\n",
    "    #     pro=lr_V2.predict_proba(feature)\n",
    "    #     print \"Label.pro:\",label,pro\n",
    "    print ('Start Co-EM LR:')\n",
    "    C = 0# 从DU中抽取置信度高的样本加入DL\n",
    "    Maxinter=20\n",
    "    print (\"从D_U中选择的置信度高的样本量,置信度C:\", C)\n",
    "    #imbalance=50 #抽取的N样本是P样本的倍数\n",
    "    #print \"抽取的Negative样本N和Positive样本数量P比例：\",imbalance\n",
    "    # N=50000\n",
    "    # P=1000\n",
    "    # print \"选择置信度高的P个D_U+和N个D_U-加入D_L:\",'P:',P,'N:',N\n",
    "    Threshold = 1e-5# 收敛阈值\n",
    "    print (\"迭代收敛条件Threshold:\" ,Threshold)# 收敛判决\n",
    "    print (\"最大迭代次数:\",Maxinter)\n",
    "    lambda_u=0.4\n",
    "    print (\"lambda_u:\",lambda_u)\n",
    "    # theta_V1_Last = 0  #收敛判断条件\n",
    "    # theta_V2_Last = 0#收敛判断条件\n",
    "    for K in range(0, Maxinter):  # K次迭代\n",
    "        print (\"******************CO-EM LR 第\" + str(K + 1) + \"次迭代开始:*****************************\")\n",
    "        print ('**************For V=1:*****************')\n",
    "\n",
    "        print (\"Start V1 View:\")\n",
    "        P_init = float(sum(y_L)) / len(y_L)\n",
    "        print ('Prior P_init:', P_init)\n",
    "        proba_D_U = lr_V2.predict_proba(V2_D_U)  #DU预测的概率\n",
    "        #print \"logreg_proba\",proba_D_U\n",
    "        proba_D_U = proba_D_U[:, 1].tolist()  # Du在Yes类上概率\n",
    "        print (\"mean of proba_D_U by v2;\",np.mean(proba_D_U))\n",
    "        print (\"max of proba_D_U by v2;\", np.max(proba_D_U))\n",
    "        print (\"median of proba_D_U by v2\",np.median(proba_D_U))\n",
    "        #proba_DU_No =logreg_proba_D_U[:,0].tolist()  #Du在No类上概率\n",
    "        #proba_DU_Yes = logreg_proba_D_U  # Du在Yes类上概率\n",
    "        # print 'proba_DU_Yes:',proba_DU_Yes\n",
    "        # Co-traing中Positive类的样本个数P和c\n",
    "        P_U = int(len(y_U) * P_init)  # Positive类的样本个数P,Du+\n",
    "        N_U = int(len(y_U) - P_U)  # Negative类的样本个数P,Du-\n",
    "        D_U_P_pro = []  # DU+pro\n",
    "        D_U_N_pro = []  # DU_pro\n",
    "        D_U_P_F = []  # DU+_\n",
    "        D_U_N_F = []  # DU_feature\n",
    "        #print 'Start:DU中P=1样本DU+:'\n",
    "        #print 'Start:DU中P=0样本DU-:'\n",
    "        # starttime1=datetime.datetime.now()\n",
    "        Dict = {}  # Item:X特征 Value:pro概率值\n",
    "        for i in range(0, len(y_U)):\n",
    "            # print 'D_U[i]:',D_U[i]\n",
    "            feature = D_U[i]\n",
    "            feature = [str(fe) for fe in feature]  # 转换为字符\n",
    "            feature = str(feature)\n",
    "            # print feature\n",
    "            pro = proba_D_U[i]\n",
    "            #print \"pro:\",pro\n",
    "            Dict.setdefault(feature, pro)\n",
    "        Dict_sort = sorted(Dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        for x in Dict_sort[0:P_U]:\n",
    "            # print x[0]\n",
    "            # print x[1]\n",
    "            feature = eval(x[0])\n",
    "            feature = [float(fe) for fe in feature]  # 转换为字符\n",
    "            pro = float(x[1])\n",
    "            D_U_P_pro.append(pro)\n",
    "            D_U_P_F.append(feature)\n",
    "            # print \"feature:\",feature\n",
    "            # print \"pro:\",pro\n",
    "        for x in Dict_sort[P_U:len(y_U)]:\n",
    "            # print x[0]\n",
    "            # print x[1]\n",
    "            feature = eval(x[0])\n",
    "            feature = [float(fe) for fe in feature]  # 转换为字符\n",
    "            pro = float(x[1])\n",
    "            D_U_N_pro.append(pro)\n",
    "            D_U_N_F.append(feature)\n",
    "        print ('DU+:', len(D_U_P_pro))\n",
    "        print ('DU_:', len(D_U_N_pro))\n",
    "        #print 'Start:DL中P=1样本DL+:'\n",
    "        #print 'Start:DL中P=0样本DL-:'\n",
    "        prediction_D_L=lr_V2.predict(V2_D_L)\n",
    "        proba_D_L=lr_V2.predict_proba(V2_D_L)\n",
    "        proba_D_L = proba_D_L[:, 1].tolist()  # Du在Yes类上概率\n",
    "        D_L_P = []  # DL+\n",
    "        D_L_N = []  # DL_\n",
    "\n",
    "        P_L = int(sum(y_L))  # Positive类的样本个数P,DL+\n",
    "        N_L = len(y_L) - P_L  # negative类的样本个数P,DL-\n",
    "        for i in range(0,len(y_L)):\n",
    "            if y_L[i]==1:\n",
    "                D_L_P.append(proba_D_L[i])\n",
    "            else:\n",
    "                D_L_N.append(proba_D_L[i])\n",
    "        print ('DL+:', len(D_L_P))\n",
    "        print ('DL_:', len(D_L_N))\n",
    "        print ('Start Estimate Parameter from Dl and Du:')\n",
    "        u_P = (float(1) / float((lambda_u*len(D_U_P_pro) + len(D_L_P)))) * (sum(D_L_P) + lambda_u*sum(D_U_P_pro))  # U+\n",
    "        u_N = (float(1) / float((lambda_u*len(D_U_N_pro) + len(D_L_N)))) * (sum(D_L_N) + lambda_u*sum(D_U_N_pro))  # U-\n",
    "        print ('parameter:u+', u_P)\n",
    "        print ('parameter:u-', u_N)\n",
    "        delta_P2List = []\n",
    "        for x in D_U_P_pro:\n",
    "            result = (float(1) / float((lambda_u*len(D_U_P_pro) + len(D_L_P)))) * lambda_u*pow(x - u_P, 2)\n",
    "            delta_P2List.append(float(result))\n",
    "        for x in D_L_P:\n",
    "            result = (float(1) / float((lambda_u*len(D_U_P_pro) + len(D_L_P)))) *lambda_u* pow(x - u_P, 2)\n",
    "            delta_P2List.append(float(result))\n",
    "        delta_P2 = sum(delta_P2List)  # delta+\n",
    "\n",
    "        print ('delta_+2:', delta_P2)\n",
    "        delta_N2List = []\n",
    "        for x in D_U_N_pro:\n",
    "            result = (float(1) / float((lambda_u*len(D_U_N_pro) + len(D_L_N))))*lambda_u*pow(x - u_N, 2)\n",
    "            delta_N2List.append(result)\n",
    "        for x in D_L_N:\n",
    "            result = (float(1) / float((lambda_u*len(D_U_N_pro) + len(D_L_N))))*lambda_u*pow(x - u_N, 2)\n",
    "            delta_N2List.append(result)\n",
    "        delta_N2 = sum(delta_N2List)  # delta+\n",
    "        print ('delta_-2:', delta_N2)\n",
    "        # N_P=1/(float(math.sqrt((delta_P2*2*math.pi))))*math.exp((-1/(2*(float(delta_P2))))*(float(1-u_P))**2)\n",
    "        # print 'N_P:',N_P  #高斯分布\n",
    "        # N_N=1/(float(math.sqrt((delta_N2*2*math.pi))))*math.exp((-1/(2*(float(delta_N2))))*(float(0-u_N))**2)\n",
    "        # print 'N_N:',N_N  #高斯分布\n",
    "        Pro_1xList = []  # EM得到的H=1的权重\n",
    "        Pro_0xList = []\n",
    "        Weight_DU = []  # DU权重\n",
    "        D_U_S = []  # DU_S选择加入V1的训练集合倒排序\n",
    "        y_U_S = []  # 对于的label\n",
    "        Weight_DU_S = []  # 对应的权重\n",
    "        print ('在V2上，选择置信度高的D_U:')  # 为了降低样本数量和噪音，选取置信度高的D_U,加入D_L\n",
    "        for I in range(0, len(D_U_P_pro)):\n",
    "            # print 'x:'+str(x)\n",
    "            # fx=lr_V2.predict_proba([x])[:,1]\n",
    "            fx =D_U_P_pro[I]\n",
    "            # fx=float(fx[0])\n",
    "            Nor_P = 1 / (float(math.sqrt((delta_P2 * 2 * math.pi)))) * math.exp(\n",
    "                (-1 / (2 * (float(delta_P2)))) * (float(fx - u_P)) ** 2)\n",
    "            Nor_N = 1 / (float(math.sqrt((delta_N2 * 2 * math.pi)))) * math.exp(\n",
    "                (-1 / (2 * (float(delta_N2)))) * (float(fx - u_N)) ** 2)\n",
    "\n",
    "            Pro_1x = Nor_P * P_init / (Nor_P * P_init + Nor_N * (1 - P_init))  # CO-EM SVM equation(8)\n",
    "            Pro_0x = 1 - Pro_1x\n",
    "            Pro_1xList.append(Pro_1x)\n",
    "            Pro_0xList.append(Pro_0x)\n",
    "            weight = abs(Pro_1x - Pro_0x)\n",
    "            Weight_DU.append(weight)\n",
    "            feature = D_U_P_F[I]\n",
    "            if weight > C:\n",
    "                D_U_S.append(feature)\n",
    "                Weight_DU_S.append(weight)\n",
    "                if Pro_1x>Pro_0x:\n",
    "                    y_U_S.append(1)\n",
    "                else:\n",
    "                    y_U_S.append(0)\n",
    "        for I in range(0, len(D_U_N_pro)):\n",
    "            # print 'x:'+str(x)\n",
    "            # fx=lr_V2.predict_proba([x])[:,1]\n",
    "            fx = D_U_N_pro[I]\n",
    "            # fx=float(fx[0])\n",
    "            Nor_P = 1 / (float(math.sqrt((delta_P2 * 2 * math.pi)))) * math.exp(\n",
    "                (-1 / (2 * (float(delta_P2)))) * (float(fx - u_P)) ** 2)\n",
    "            Nor_N = 1 / (float(math.sqrt((delta_N2 * 2 * math.pi)))) * math.exp(\n",
    "                (-1 / (2 * (float(delta_N2)))) * (float(fx - u_N)) ** 2)\n",
    "\n",
    "            Pro_1x = Nor_P * P_init / (Nor_P * P_init + Nor_N * (1 - P_init))  # CO-EM SVM equation(8)\n",
    "            Pro_0x = 1 - Pro_1x\n",
    "            Pro_1xList.append(Pro_1x)\n",
    "            Pro_0xList.append(Pro_0x)\n",
    "            weight = abs(Pro_1x - Pro_0x)\n",
    "            #weight=Pro_1x\n",
    "            Weight_DU.append(weight)\n",
    "            feature = D_U_N_F[I]\n",
    "            if  weight>C:\n",
    "                D_U_S.append(feature)\n",
    "                Weight_DU_S.append(weight)\n",
    "                if Pro_1x>Pro_0x:\n",
    "                    y_U_S.append(1)\n",
    "                else:\n",
    "                    y_U_S.append(0)\n",
    "        # 选择D_U,weight高的若干分类，并在V1上加入D_U,label,weight\n",
    "        Weight_DU=array(Weight_DU)\n",
    "        # print \"mean of Pro_1_Du;\", np.mean(Pro_1xList)\n",
    "        # print \"max of Pro_1_Du;\", np.max(Pro_1xList)\n",
    "        # print \"mdedian of Pro_1_Du;\",np.median(Pro_1xList)\n",
    "\n",
    "        D_U_S = array(D_U_S)  #置信度大于C的样本\n",
    "        Weight_DU_S=array(Weight_DU_S)\n",
    "        V2_D_U_S = array(D_U_S[:, S:E])\n",
    "        # print \"sharp of V2_D_U_S:\", array(V2_D_U_S).shape\n",
    "        #prediction_D_U_S, logreg_proba_D_U_S = SGDLRPrediction(V2_D_U_S, theta2)  # Sigmod预测结果极其概率\n",
    "        print(\"对未标记样本D_U，分类完成，待加入标记样本D_L:\")\n",
    "        #print '初步The scale of y_U_S(加入的D_U):',D_U_S.shape[0]\n",
    "        print ('The scale of 待加入的D_U:', Counter(y_U_S))\n",
    "        #采样达到均衡：\n",
    "        D_U_S_P=[]#正类样本\n",
    "        D_U_S_N=[]#负类样本\n",
    "        y_U_S_P=[]\n",
    "        y_U_S_N=[]\n",
    "        Weight_DU_S_P=[]\n",
    "        Weight_DU_S_N=[]\n",
    "        for i in range(len(y_U_S)):\n",
    "            if y_U_S[i]==1:\n",
    "                y_U_S_P.append(1)\n",
    "                D_U_S_P.append(D_U_S[i])\n",
    "                Weight_DU_S_P.append(Weight_DU_S[i])\n",
    "            else:\n",
    "                y_U_S_N.append(0)\n",
    "                D_U_S_N.append(D_U_S[i])\n",
    "                Weight_DU_S_N.append(Weight_DU_S[i])\n",
    "        #N =  int(len(y_U_S_P)/P_init)# 选取的N样本是P类样本数量\n",
    "        try:\n",
    "            print (\"mean of Weight_DU_S_N;\",np.mean(Weight_DU_S_N))\n",
    "            print (\"mean of Weight_DU_S_P;\", np.mean(Weight_DU_S_P))\n",
    "            print (\"median of Weight_DU_S_N;\",np.median(Weight_DU_S_N))\n",
    "            print (\"median of Weight_DU_S_P;\", np.median(Weight_DU_S_P))\n",
    "            print (\"max of Weight_DU_S_N;\",np.max(Weight_DU_S_N))\n",
    "            print (\"max of Weight_DU_S_P;\", np.max(Weight_DU_S_P))\n",
    "            print (\"min of Weight_DU_S_N;\",np.min(Weight_DU_S_N))\n",
    "            print (\"min of Weight_DU_S_P;\", np.min(Weight_DU_S_P))\n",
    "        except:\n",
    "            print (\"Non-Purchase Label\")\n",
    "\n",
    "        # print 'sharp of D_L:',shape(D_L)\n",
    "        # print 'sharp of D_U_S:',shape(D_U_S)\n",
    "        D_new = np.vstack((D_L, D_U_S))\n",
    "        D_new_V1 = D_new[:, 0:S]\n",
    "        # print 'sharp of y_L:',shape(y_L)\n",
    "        # print 'sharp of y_U_S:',shape(y_U_S)\n",
    "        # print 'y_L:',y_L\n",
    "        # print 'y_U_S:',array(y_U_S)\n",
    "        Weight_DL = np.ones(len(D_L), dtype=np.float)\n",
    "        Weight_new = np.hstack((Weight_DL, Weight_DU_S))\n",
    "        D_new = np.vstack((D_L, D_U_S))\n",
    "        D_new_V1 = D_new[:, 0:S]\n",
    "        # print 'sharp of y_L:',shape(y_L)\n",
    "        # print 'sharp of y_U_S:',shape(y_U_S)\n",
    "        # print 'y_L:',y_L\n",
    "        # print 'y_U_S:',array(y_U_S)\n",
    "        y_new = np.hstack((y_L, y_U_S))\n",
    "        print (\"The scale of y_new:\",Counter(y_new))\n",
    "        # 部分样本的V1未必全，因为下面进行样本选择，条件为#\n",
    "        print ('Start Classify on V1 for DL +D_U with weight by LR:')\n",
    "        #lr_V1 = LogisticRegression(C=1e5, solver='newton-cg', multi_class='ovr', class_weight='balanced')  # LR参数设置\n",
    "        lr_V1 = LogisticRegression(C=1e5, solver='sag', multi_class='ovr',tol=1e-4,max_iter=10000,class_weight={0: 0.01, 1:0.08})  # LR参数设置final\n",
    "        #lr_V1 = SGDClassifier(loss=\"log\", penalty=\"l2\", tol=1e-4, max_iter=5000,class_weight={0: 0.01, 1: 0.05})  # SGD梯度下降\n",
    "        #lr_V1 = LogisticRegression(C=1e5, solver='newton-cg', multi_class='ovr')\n",
    "        D_new_V1_W=[]\n",
    "        y_new_V1=[]\n",
    "        UsertypeList=[]\n",
    "        #print \"D_new_V1.shape[0]:\",D_new_V1.shape[0]\n",
    "        for i in range(0,D_new_V1.shape[0]):#加入权重\n",
    "            #样本选择，选择user为1和2的用户\n",
    "            featureList=D_new_V1[i, 0:S]\n",
    "            Usertype=featureList[S-1]\n",
    "            #print 'Usertype:',Usertype\n",
    "            UsertypeList.append(Usertype)\n",
    "            if round(Usertype, 2) != round(FirstUser_std, 2):  # 只加入Recent-User到V1中\n",
    "            #if round(Usertype,2)==round(Recent_std,2):#只加入Recent-User到V1中\n",
    "                X_list=D_new_V1[i,0:S-1]*Weight_new[i]\n",
    "                #print X_list\n",
    "                D_new_V1_W.append(X_list)\n",
    "                y_new_V1.append(y_new[i])\n",
    "        #print 'Counter(Usertype):',Counter(UsertypeList)\n",
    "        print ('样本选择后View V1上训练集合:', Counter(y_new_V1))\n",
    "        lr_V1.fit(D_new_V1_W, y_new_V1)\n",
    "        #print \"lr_V1:\",lr_V1\n",
    "        theta_X_V1=array(lr_V1.coef_[0])\n",
    "        theta_n_V1=array(lr_V1.intercept_)\n",
    "        #print theta_X_V1,theta_n_V1\n",
    "        theta_V1=np.hstack((theta_X_V1,theta_n_V1))\n",
    "\n",
    "        # for i in range(0,len(CFproList)):\n",
    "        #     label=y_test_v1[i]\n",
    "        #     pro=CFproList[i]\n",
    "        #     if label==1:\n",
    "        #         print \"Label.pro:\",label,pro\n",
    "        # clf = RandomForestClassifier(max_depth=2, random_state=42, max_features='log2')  # 分类器参数设置\n",
    "        # #clf.fit(train_X_sample, train_y_sample)  # 训练RF\n",
    "        # clf.fit(D_new_V1_W, y_new_V1)  # 训练RF\n",
    "        # #print 'RF Creating Finished!'\n",
    "        # #print \"RF feature importances:\"\n",
    "        # print 'Feature Importance by RF:'\n",
    "        # print(clf.feature_importances_)  # 特征重要性\n",
    "        # D_L=D_new #新的训练样本 D_L不变\n",
    "        # y_L=y_new  #新的训练样本\n",
    "        # V1_D_L = array(D_L[:, 0:S])\n",
    "        # V2_D_L = array(D_L[:, S:E])\n",
    "        print ('*******************For V=2:*******************')\n",
    "        print (\"Start V View:\")\n",
    "        P_init = float(sum(y_L)) / len(y_L)\n",
    "        print ('Prior P_init:', P_init)\n",
    "        D_U_ER=[]  #ever and recent user 特征矩阵\n",
    "        y_U_ER=[]\n",
    "        for i in range(0,D_U.shape[0]):\n",
    "            featureList=D_U[i, 0:S]\n",
    "            Usertype=featureList[S-1]\n",
    "            #print 'Usertype:',Usertype\n",
    "            if round(Usertype, 2) != round(FirstUser_std, 2):  # 只加入Recent-User到V1中\n",
    "                feature = D_U[i]  #DU预测的概率\n",
    "                D_U_ER.append(feature)\n",
    "            #print \"logreg_proba\",proba_D_U\n",
    "        print (\"sharp of D_U_ER:\",array(D_U_ER).shape[0])\n",
    "        proba_D_U_ER=lr_V1.predict_proba(array(D_U_ER)[:,0:S-1])\n",
    "        proba_D_U = proba_D_U_ER[:, 1].tolist()  # Du在Yes类上概率\n",
    "        print (\"mean of proba_D_U by v2;\",np.mean(proba_D_U))\n",
    "        print (\"max of proba_D_U by v2;\", np.max(proba_D_U))\n",
    "        print (\"median of proba_D_U by v2\",np.median(proba_D_U))\n",
    "        # Co-traing中Positive类的样本个数P和c\n",
    "        P_U = int(len(proba_D_U) * P_init)  # Positive类的样本个数P,Du+\n",
    "        N_U = int(len(proba_D_U) - P_U)  # Negative类的样本个数P,Du-\n",
    "        #print \"P_U and N_U:\",P_U,N_U\n",
    "        D_U_P = []  # DU+\n",
    "        D_U_N = []  # DU_\n",
    "        D_U_P_F = []  # DU+_\n",
    "        D_U_N_F = []  # DU_feature\n",
    "        print ('Start:DU中P=1样本DU+:')\n",
    "        print ('Start:DU中P=0样本DU-:')\n",
    "        # starttime1=datetime.datetime.now()\n",
    "        Dict = {}  # Item:X特征 Value:pro概率值\n",
    "        for i in range(0, len(proba_D_U)):\n",
    "            # print 'D_U[i]:',D_U[i]\n",
    "            feature = D_U_ER[i]\n",
    "            feature = [str(fe) for fe in feature]  # 转换为字符\n",
    "            feature = str(feature)\n",
    "            # print feature\n",
    "            pro = proba_D_U[i]\n",
    "            #print \"pro:\",pro\n",
    "            Dict.setdefault(feature, pro)\n",
    "        Dict_sort = sorted(Dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        for x in Dict_sort[0:P_U]:\n",
    "            # print x[0]\n",
    "            # print x[1]\n",
    "            feature = eval(x[0])\n",
    "            feature = [float(fe) for fe in feature]  # 转换为字符\n",
    "            pro = float(x[1])\n",
    "            D_U_P.append(pro)\n",
    "            D_U_P_F.append(feature)\n",
    "            # print \"feature:\",feature\n",
    "            # print \"pro:\",pro\n",
    "        for x in Dict_sort[P_U:len(proba_D_U)]:\n",
    "            # print x[0]\n",
    "            # print x[1]\n",
    "            feature = eval(x[0])\n",
    "            feature = [float(fe) for fe in feature]  # 转换为字符\n",
    "            pro = float(x[1])\n",
    "            D_U_N.append(pro)\n",
    "            D_U_N_F.append(feature)\n",
    "\n",
    "        print ('DU+:', len(D_U_P))\n",
    "        print ('DU_:', len(D_U_N))\n",
    "        prediction_D_L=lr_V1.predict(V1_D_L[:,0:S-1])\n",
    "        proba_D_L=lr_V1.predict_proba(V1_D_L[:,0:S-1])\n",
    "        proba_D_L = proba_D_L[:, 1].tolist()  # Du在Yes类上概率\n",
    "        D_L_P = []  # DL+\n",
    "        D_L_N = []  # DL_\n",
    "        P_L = int(sum(y_L))  # Positive类的样本个数P,DL+\n",
    "        N_L = len(y_L) - P_L  # negative类的样本个数P,DL-\n",
    "        for i in range(0,len(y_L)):\n",
    "            if y_L[i]==1:\n",
    "                D_L_P.append(proba_D_L[i])\n",
    "            else:\n",
    "                D_L_N.append(proba_D_L[i])\n",
    "        print ('DL+:', len(D_L_P))\n",
    "        print ('DL_:', len(D_L_N))\n",
    "        print ('Start Estimate Parameter from Dl and Du:')\n",
    "        u_P = (float(1) / float((len(D_U_P) + len(D_L_P)))) * (sum(D_L_P) + sum(D_U_P))  # U+\n",
    "        u_N = (float(1) / float((len(D_U_N) + len(D_L_N)))) * (sum(D_L_N) + sum(D_U_N))  # U-\n",
    "        print ('parameter:u+', u_P)\n",
    "        print ('parameter:u-', u_N)\n",
    "        delta_P2List = []\n",
    "        for x in D_U_P:\n",
    "            result = (float(1) / float((len(D_U_P) + len(D_L_P)))) * pow(x - u_P, 2)\n",
    "            delta_P2List.append(result)\n",
    "        for x in D_L_P:\n",
    "            result = (float(1) / float((len(D_U_P) + len(D_L_P)))) * pow(x - u_P, 2)\n",
    "            delta_P2List.append(result)\n",
    "        delta_P2 = sum(delta_P2List)  # delta+\n",
    "\n",
    "        print ('delta_+2:', delta_P2)\n",
    "        delta_N2List = []\n",
    "        for x in D_U_N:\n",
    "            result = (float(1) / float((len(D_U_N) + len(D_L_N)))) * pow(x - u_N, 2)\n",
    "            delta_N2List.append(result)\n",
    "        for x in D_L_N:\n",
    "            result = (float(1) / float((len(D_U_N) + len(D_L_N)))) * pow(x - u_N, 2)\n",
    "            delta_N2List.append(result)\n",
    "        delta_N2 = sum(delta_N2List)  # delta+\n",
    "        print ('delta_-2:', delta_N2)\n",
    "        # N_P=1/(float(math.sqrt((delta_P2*2*math.pi))))*math.exp((-1/(2*(float(delta_P2))))*(float(1-u_P))**2)\n",
    "        # print 'N_P:',N_P  #高斯分布\n",
    "        # N_N=1/(float(math.sqrt((delta_N2*2*math.pi))))*math.exp((-1/(2*(float(delta_N2))))*(float(0-u_N))**2)\n",
    "        # print 'N_N:',N_N  #高斯分布\n",
    "        Pro_1xList = []  # EM得到的H=1的权重\n",
    "        Pro_0xList = []\n",
    "        Weight_DU = []  # DU权重\n",
    "        D_U_S = []  # DU_S选择加入V1的训练集合倒排序\n",
    "        y_U_S = []  # 对于的label\n",
    "        Weight_DU_S = []  # 对应的权重\n",
    "        #print \"len(D_U_P_pro)\",len(D_U_P_pro)\n",
    "        #print \"len(D_U_P_F)\",len(D_U_P_F)\n",
    "        for I in range(0, len(D_U_P_F)):\n",
    "            # print 'x:'+str(x)\n",
    "            # fx=lr_V2.predict_proba([x])[:,1]\n",
    "            fx = D_U_P[I]\n",
    "            # fx=float(fx[0])\n",
    "            Nor_P = 1 / (float(math.sqrt((delta_P2 * 2 * math.pi)))) * math.exp(\n",
    "                (-1 / (2 * (float(delta_P2)))) * (float(fx - u_P)) ** 2)\n",
    "            Nor_N = 1 / (float(math.sqrt((delta_N2 * 2 * math.pi)))) * math.exp(\n",
    "                (-1 / (2 * (float(delta_N2)))) * (float(fx - u_N)) ** 2)\n",
    "\n",
    "            Pro_1x = Nor_P * P_init / (Nor_P * P_init + Nor_N * (1 - P_init))  # CO-EM SVM equation(8)\n",
    "            Pro_0x = 1 - Pro_1x\n",
    "            Pro_1xList.append(Pro_1x)\n",
    "            Pro_0xList.append(Pro_0x)\n",
    "            weight = abs(Pro_1x - Pro_0x)\n",
    "            Weight_DU.append(weight)\n",
    "            feature =  D_U_P_F[I]\n",
    "            if weight > C:\n",
    "                D_U_S.append(feature)\n",
    "                Weight_DU_S.append(weight)\n",
    "                if Pro_1x>Pro_0x:\n",
    "                    y_U_S.append(1)\n",
    "                else:\n",
    "                    y_U_S.append(0)\n",
    "        #print \"D_U_N:\",len(D_U_N_F)\n",
    "        for I in range(0, len(D_U_N)):\n",
    "            # print 'x:'+str(x)\n",
    "            # fx=lr_V2.predict_proba([x])[:,1]\n",
    "            fx = D_U_N[I]\n",
    "            # fx=float(fx[0])\n",
    "            Nor_P = 1 / (float(math.sqrt((delta_P2 * 2 * math.pi)))) * math.exp(\n",
    "                (-1 / (2 * (float(delta_P2)))) * (float(fx - u_P)) ** 2)\n",
    "            Nor_N = 1 / (float(math.sqrt((delta_N2 * 2 * math.pi)))) * math.exp(\n",
    "                (-1 / (2 * (float(delta_N2)))) * (float(fx - u_N)) ** 2)\n",
    "\n",
    "            Pro_1x = Nor_P * P_init / (Nor_P * P_init + Nor_N * (1 - P_init))  # CO-EM SVM equation(8)\n",
    "            Pro_0x = 1 - Pro_1x\n",
    "            Pro_1xList.append(Pro_1x)\n",
    "            Pro_0xList.append(Pro_0x)\n",
    "            weight = abs(Pro_1x - Pro_0x)\n",
    "            # weight=Pro_1x\n",
    "            Weight_DU.append(weight)\n",
    "            feature = D_U_N_F[I]\n",
    "            if weight > C:\n",
    "                D_U_S.append(feature)\n",
    "                Weight_DU_S.append(weight)\n",
    "                if Pro_1x>Pro_0x:\n",
    "                    y_U_S.append(1)\n",
    "                else:\n",
    "                    y_U_S.append(0)\n",
    "        # 选择D_U,weight高的若干分类，并在V1上加入D_U,label,weight\n",
    "        D_U_S = array(D_U_S)\n",
    "        Weight_DU_S=array(Weight_DU_S)\n",
    "        # print \"sharp of V2_D_U_S:\", array(V2_D_U_S).shape\n",
    "        #prediction_D_U_S, logreg_proba_D_U_S = SGDLRPrediction(V2_D_U_S, theta2)  # Sigmod预测结果极其概率\n",
    "        print (\"对未标记样本D_U，分类完成，待加入标记样本D_L:\")\n",
    "        print ('The scale of y_U_S(加入分类后的D_U):', Counter(y_U_S))\n",
    "        #采样达到均衡：\n",
    "        D_U_S_P=[]#正类样本\n",
    "        D_U_S_N=[]#负类样本\n",
    "        y_U_S_P=[]\n",
    "        y_U_S_N=[]\n",
    "        Weight_DU_S_P=[]\n",
    "        Weight_DU_S_N=[]\n",
    "        for i in range(len(y_U_S)):\n",
    "            if y_U_S[i]==1:\n",
    "                y_U_S_P.append(1)\n",
    "                D_U_S_P.append(D_U_S[i])\n",
    "                Weight_DU_S_P.append(Weight_DU_S[i])\n",
    "            else:\n",
    "                y_U_S_N.append(0)\n",
    "                D_U_S_N.append(D_U_S[i])\n",
    "                Weight_DU_S_N.append(Weight_DU_S[i])\n",
    "        try:\n",
    "            print (\"mean of Weight_DU_S_N;\",np.mean(Weight_DU_S_N))\n",
    "            print (\"mean of Weight_DU_S_P;\", np.mean(Weight_DU_S_P))\n",
    "            print (\"median of Weight_DU_S_N;\",np.median(Weight_DU_S_N))\n",
    "            print (\"median of Weight_DU_S_P;\", np.median(Weight_DU_S_P))\n",
    "            print (\"max of Weight_DU_S_N;\",np.max(Weight_DU_S_N))\n",
    "            print (\"max of Weight_DU_S_P;\", np.max(Weight_DU_S_P))\n",
    "            print (\"min of Weight_DU_S_N;\",np.min(Weight_DU_S_N))\n",
    "            print (\"min of Weight_DU_S_P;\", np.min(Weight_DU_S_P))\n",
    "        except:\n",
    "            print (\"Non-Purchase Label\")\n",
    "        #print 'sharp of D_L:',shape(D_L)\n",
    "        #print 'sharp of D_U_S:',shape(D_U_S)\n",
    "        D_new = np.vstack((D_L, D_U_S))\n",
    "        D_new_V2 = D_new[:, S:E]\n",
    "        # print 'sharp of y_L:',shape(y_L)\n",
    "        # print 'sharp of y_U_S:',shape(y_U_S)\n",
    "        # print 'y_L:',y_L\n",
    "        # print 'y_U_S:',array(y_U_S)\n",
    "        y_new = np.hstack((y_L, y_U_S))\n",
    "        Weight_DL = np.ones(len(D_L), dtype=np.float)\n",
    "        Weight_new = np.hstack((Weight_DL, Weight_DU_S))\n",
    "        print (\"The scale of y_new:\",Counter(y_new))\n",
    "        # 部分样本的V1未必全，因为下面进行样本选择，条件为#\n",
    "        print ('Start Classify on V2 for DL +D_U with weight by LR:')\n",
    "        #lr_V2 = LogisticRegression(C=1e5, solver='sag', multi_class='ovr')\n",
    "        lr_V2 = LogisticRegression(C=1e5, solver='sag', tol=1e-5,max_iter=5000,multi_class='ovr',class_weight={0: 0.01, 1: 0.03})  # LR参数设置final\n",
    "        #lr_V2 = LogisticRegression(C=1e5, solver='sag', tol=1e-5, max_iter=5000, multi_class='ovr',class_weight={0: 0.01, 1: 0.03})  # LR参数设置\n",
    "        #lr_V2 = SGDClassifier(loss=\"log\", penalty=\"l2\", tol=1e-4, max_iter=5000,class_weight={0: 0.01, 1: 0.04})  # SGD梯度下降\n",
    "        #lr_V2 = LogisticRegression(C=1e5, solver='sag', multi_class='ovr',class_weight='balanced')  # LR参数设置\n",
    "        D_new_V2_W=[]\n",
    "        for i in range(0,D_new_V2.shape[0]):#加入权重\n",
    "            X_list=D_new_V2[i,:]*Weight_new[i]\n",
    "            #print X_list\n",
    "            D_new_V2_W.append(X_list)\n",
    "        print ('View V2上训练集合:', Counter(y_new))\n",
    "        lr_V2.fit(D_new_V2_W, y_new)\n",
    "        theta_X_V2=lr_V2.coef_[0]\n",
    "        theta_n_V2=lr_V2.intercept_\n",
    "        theta_V2=np.hstack((theta_X_V2,theta_n_V2))\n",
    "        #print \"lr_V2:\",lr_V2\n",
    "\n",
    "        print ('Lr_V1 Coef:'+str(lr_V1.coef_))\n",
    "        print ('Lr_V1 intercept_:'+str(lr_V1.intercept_))\n",
    "        print ('Lr_V1 n_iter_:'+str(lr_V1.n_iter_))\n",
    "        CFprediction=[]\n",
    "        CFproList=[]\n",
    "        y_test_v1=[]\n",
    "        for i in range(V1_D_Test.shape[0]):\n",
    "            feature_V1=V1_D_Test[i,:]\n",
    "            #print \"feature_v1\",feature_V1\n",
    "            #print \"feayure_V2\",feature_V2\n",
    "            if round(feature_V1[S-1],2)!=round(FirstUser_std,2):#只加入Recent-User到V1中\n",
    "                feature_V1=array(feature_V1[0:S-1]).reshape(1,-1)\n",
    "                pro=lr_V1.predict_proba(feature_V1)\n",
    "                pro=pro[0,1]\n",
    "                CFprediction.append(int(round(pro)))\n",
    "                CFproList.append(pro)\n",
    "                y_test_v1.append(y_test[i])\n",
    "                #print \"Label.pro:\", y_test[i], pro\n",
    "        print (\"Test集合V1的预测效果:\")\n",
    "        print(classification_report(y_test_v1, CFprediction))\n",
    "\n",
    "        print ('Lr_V2 Coef:'+str(lr_V2.coef_))\n",
    "        print ('Lr_V2 intercept_:'+str(lr_V2.intercept_))\n",
    "        print ('Lr_V2 n_iter_:'+str(lr_V2.n_iter_))\n",
    "        prediction_V2_Test=lr_V2.predict(V2_D_Test)\n",
    "        proba_V2_Test=lr_V2.predict_proba(V2_D_Test)\n",
    "        print (\"Test集合V2的预测效果:\")\n",
    "        print(classification_report(y_test, prediction_V2_Test))\n",
    "        # D_L=D_new #加入新的样本后的DL训练样本  #D_L不变\n",
    "        # y_L=y_new  #加入新的样本后的DL训练样本\n",
    "        # V1_D_L = array(D_L[:, 0:S])\n",
    "        # V2_D_L = array(D_L[:, S:E])\n",
    "        # 对测试集合V1,V2进行预测CombineFunction\n",
    "        CFprediction=[]\n",
    "        CFproList=[]\n",
    "        CFprediction_F = []  # CombineFunction\n",
    "        CFproList_F = []\n",
    "        y_test_F = []\n",
    "        CFprediction_E = []  # CombineFunction\n",
    "        CFproList_E = []\n",
    "        y_test_E = []\n",
    "        CFprediction_R = []  # CombineFunction\n",
    "        CFproList_R = []\n",
    "        y_test_R = []\n",
    "        Combine=0 #是否采用联合分类器,1采样,不采用\n",
    "        if Combine==1:\n",
    "            for i in range(V1_D_Test.shape[0]):\n",
    "                feature_V1=V1_D_Test[i,:]\n",
    "                feature_V2=V2_D_Test[i,:]\n",
    "                #print \"feature_v1\",feature_V1\n",
    "                #print \"feayure_V2\",feature_V2\n",
    "                if round(feature_V1[S-1],2)==round(FirstUser_std,2):#初次访问用户,只有f—V2\n",
    "                    feature_V2=array(feature_V2).reshape(1,-1)\n",
    "                    pro=lr_V2.predict_proba(feature_V2)\n",
    "                    pro=pro[0,1]\n",
    "                    CFprediction_F.append(int(round(pro)))\n",
    "                    CFproList_F.append(pro)\n",
    "                    y_test_F.append(y_test[i])\n",
    "                elif round(feature_V1[S - 1], 2) == round(Evervisit_std, 2):\n",
    "                    h_V1 = Hypothesis(feature_V1[0:S - 1], theta_V1)\n",
    "                    h_V2 = Hypothesis(feature_V2, theta_V2)\n",
    "                    pro = 0.5*(Sigmoid(h_V1- theta_V1[-1])+Sigmoid( h_V2))\n",
    "                    CFprediction_E.append(int(round(pro)))\n",
    "                    CFproList_E.append(pro)\n",
    "                    y_test_E.append(y_test[i])\n",
    "                elif round(feature_V1[S - 1], 2) == round(Recent_std, 2):\n",
    "                    feature_V1 = array(feature_V1[0:S-1]).reshape(1, -1)\n",
    "                    feature_V2 = array(feature_V2).reshape(1,-1)\n",
    "                    #print \"feature_V2\",feature_V2\n",
    "                    pro_V1=lr_V1.predict_proba(feature_V1)\n",
    "                    pro_V1=pro_V1[0,1]\n",
    "                    pro_V2=lr_V2.predict_proba(feature_V2)\n",
    "                    pro_V2 = pro_V2[0,1]\n",
    "                    pro=0.5*(pro_V1+pro_V2)\n",
    "                    CFprediction_R.append(int(round(pro)))\n",
    "                    CFproList_R.append(pro)\n",
    "                    y_test_R.append(y_test[i])\n",
    "                #print \"pro and label:\",round(pro),y_test[i]\n",
    "                CFprediction.append(int(round(pro)))\n",
    "                CFproList.append(pro)\n",
    "        else:\n",
    "            #分别计算三类用户的性能\n",
    "            for i in range(V1_D_Test.shape[0]):\n",
    "                feature_V1 = V1_D_Test[i, :]\n",
    "                feature_V2 = V2_D_Test[i, :]\n",
    "                if round(feature_V1[S - 1], 2) == round(FirstUser_std, 2):  # 初次访问用户,只有f—V2\n",
    "                    h_V2 = Hypothesis(feature_V2, theta_V2)\n",
    "                    # print 'h_V2:',h_V2\n",
    "                    pro = Sigmoid(h_V2)\n",
    "                    # if y_test[i]==1:\n",
    "                    #     print \"feature_v2\", feature_V2[0:S - 1]\n",
    "                    #     print \"pro:\", pro\n",
    "                    CFprediction_F.append(int(round(pro)))\n",
    "                    CFproList_F.append(pro)\n",
    "                    y_test_F.append(y_test[i])\n",
    "                elif round(feature_V1[S - 1], 2) == round(Evervisit_std, 2):\n",
    "                    h_V1 = Hypothesis(feature_V1[0:S - 1], theta_V1)\n",
    "                    h_V2 = Hypothesis(feature_V2, theta_V2)\n",
    "                    h_V1_V2 = 1 * (h_V1 + h_V2)\n",
    "                    pro = Sigmoid(h_V1_V2)\n",
    "                    #if y_test[i] == 1:\n",
    "                        # print \"feature_v1\", feature_V1[0:S - 1]\n",
    "                        # print \"feayure_V2\", feature_V2\n",
    "                        # print \"h_V2:\",h_V2\n",
    "                        # print \"h_V1,h_V2,h_V1_V2:\", h_V1, h_V2, h_V1_V2\n",
    "                        # print \"pro:\", pro\n",
    "                    CFprediction_E.append(int(round(pro)))\n",
    "                    CFproList_E.append(pro)\n",
    "                    y_test_E.append(y_test[i])\n",
    "                elif round(feature_V1[S - 1], 2) == round(Recent_std, 2):\n",
    "                    # print \"feature_v1\",feature_V1\n",
    "                    # print \"feayure_V2\",feature_V2\n",
    "                    h_V1 = Hypothesis(feature_V1[0:S - 1], theta_V1)\n",
    "                    h_V2 = Hypothesis(feature_V2, theta_V2)\n",
    "                    h_V1_V2 = 1 * (h_V1 + h_V2)\n",
    "                    # print \"h_V1,h_V2,h_V1_V2:\",h_V1,h_V2,h_V1_V2\n",
    "                    # print \"pro and label:\",round(pro),y_test[i]\n",
    "                    pro = Sigmoid(h_V1_V2)\n",
    "                    # print \"pro:\",pro\n",
    "                    # pro = Sigmoid(h_V2)\n",
    "                    # if y_test[i]==1:\n",
    "                    #     print \"feature_v1\", feature_V1[0:S - 1]\n",
    "                    #     print \"feayure_V2\", feature_V2\n",
    "                    #     print \"h_V1,h_V2,h_V1_V2:\",h_V1,h_V2,h_V1_V2\n",
    "                    #     print \"pro and label:\",pro,y_test[i]\n",
    "                    CFprediction_R.append(int(round(pro)))\n",
    "                    CFproList_R.append(pro)\n",
    "                    y_test_R.append(y_test[i])\n",
    "                CFprediction.append(int(round(pro)))\n",
    "                CFproList.append(pro)\n",
    "        print (\"First-visit User,Ever-visit User和Recent-visit User加入V2性能提升情况:\")\n",
    "        print(\"Combined Function预测效果 For First_visit Users:\")\n",
    "        print(classification_report(y_test_F, CFprediction_F))\n",
    "        AUC = metrics.roc_auc_score(y_test_F, CFproList_F)\n",
    "        print ('AUC:', AUC)\n",
    "        print('\\n')\n",
    "        print(\"Combined Function预测效果 For Ever_visit Users:\")\n",
    "        print(classification_report(y_test_E, CFprediction_E))\n",
    "        AUC = metrics.roc_auc_score(y_test_E, CFproList_E)\n",
    "        print ('AUC:', AUC)\n",
    "        print('\\n')\n",
    "        print(\"Combined Function预测效果 For Recent_visit Users:\")\n",
    "        print(classification_report(y_test_R, CFprediction_R))\n",
    "        AUC = metrics.roc_auc_score(y_test_R, CFproList_R)\n",
    "        print ('AUC:', AUC)\n",
    "        print('\\n')\n",
    "        print(\"Combined Function预测效果 For All Users:\")\n",
    "        print(classification_report(y_test, CFprediction))\n",
    "        #print 'CFprediction:',CFprediction\n",
    "        AUC = metrics.roc_auc_score(y_test, CFproList)\n",
    "        print ('AUC:',AUC)\n",
    "        file = '/ProAndLabel_D1.csv'\n",
    "        csvfile = open(Dir + file, 'wb')\n",
    "        import codecs\n",
    "        csvfile.write(codecs.BOM_UTF8)\n",
    "        writer = csv.writer(csvfile)\n",
    "        for ID in range(0, len(y_test)):\n",
    "            #print CFproList[ID], y_test[ID]\n",
    "            writer.writerow([CFproList[ID], y_test[ID]])\n",
    "        csvfile.close()\n",
    "        #print(classification_report(y_test, prediction_V2_Test))\n",
    "        #print \"theta_V1:\",theta_V1\n",
    "        #print \"theta_V2:\", theta_V2\n",
    "        if K>0: #判决收敛\n",
    "            T1=theta_V1-theta_V1_Last\n",
    "            T2=theta_V2-theta_V2_Last\n",
    "            print (\"max T1:\",max(T1))\n",
    "            print (\"max T2:\",max(T2))\n",
    "            T=max(max(T1),max(T2))\n",
    "            if float(T)<Threshold:\n",
    "                print (\"******************CO-EM LR 第\" + str(K + 1) + \"次迭代结束并收敛:*****************************\")\n",
    "                break\n",
    "        theta_V1_Last =theta_V1  # 收敛判断条件\n",
    "        theta_V2_Last =theta_V2 # 收敛判断条件\n",
    "        endtime = datetime.datetime.now()\n",
    "        dwell = (endtime - starttime).seconds\n",
    "        print ('Till Now Run dwell time:' + str(int(dwell/60)) + 'Minutes')\n",
    "        print (\"******************CO-EM LR 第\" + str(K + 1) + \"次迭代结束:*****************************\")\n",
    "        print ('\\n')\n",
    "        #print \"联合V1和V2效果,Combined Funtions:\"\n",
    "        #prediction_V1_V2_Test = [round(0.5 * (proba_V1_Test[i] + proba_V2_Test[i])) for i in range(len(proba_V2_Test))]\n",
    "        #print(classification_report(y_test, prediction_V1_V2_Test))\n",
    "\n",
    "    print (\"Lr_V1参数配置：\", lr_V1)\n",
    "    print (\"Lr_V2参数配置：\", lr_V2)\n",
    "    print (\"******************达到最大迭代次数，CO-EM LR 第\" + str(K + 1) + \"次迭代结束:*****************************\")\n",
    "    print('\\n')\n",
    "    # print(classification_report(y_test, predictionnew))\n",
    "    #clf = RandomForestClassifier(max_depth=2, random_state=42, max_features='log2')  # 分类器参数设置\n",
    "    # clf.fit(train_X_sample, train_y_sample)  # 训练RF\n",
    "    #clf.fit(X_train_std, y_train)  # 训练RF\n",
    "    #print 'Feature Importance by RF:'\n",
    "    #print(clf.feature_importances_)  # 特征重要性\n",
    "    endtime = datetime.datetime.now()\n",
    "    dwell = (endtime - starttime).seconds\n",
    "    print ('Run dwell time:' + str(int(dwell / 60)) + 'Minutes')\n",
    "LRClassifier('data1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3201f4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
