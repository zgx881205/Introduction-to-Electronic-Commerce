{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris数据集前5行：\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "Iris数据集前5行分类：\n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "print('Iris数据集前5行：')\n",
    "print(iris.data[0:5,:])\n",
    "print('Iris数据集前5行分类：')\n",
    "print(iris.target[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris数据集归一化处理后前5行：\n",
      "[[0.22222222 0.625      0.06779661 0.04166667]\n",
      " [0.16666667 0.41666667 0.06779661 0.04166667]\n",
      " [0.11111111 0.5        0.05084746 0.04166667]\n",
      " [0.08333333 0.45833333 0.08474576 0.04166667]\n",
      " [0.19444444 0.66666667 0.06779661 0.04166667]]\n"
     ]
    }
   ],
   "source": [
    "#预处理阶段\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#区间缩放，返回值为缩放到[0, 1]区间的数据\n",
    "iris_data=MinMaxScaler().fit_transform(iris.data)\n",
    "print('Iris数据集归一化处理后前5行：')\n",
    "print(iris_data[0:5,:])\n",
    "iris_df=pd.DataFrame(iris_data,columns=['Sepal Length',\n",
    "                                        'Sepal Width','Petal Length','Petal Width'])\n",
    "iris_df['target']=iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'comb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7ac2a34fc4b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m#训练集与测试集分离阶段\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m X_train, X_test, y_train, y_test = train_test_split(iris_df.iloc[:,0:4], \n\u001b[1;32m      4\u001b[0m             iris_df['target'], random_state= 14)\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Iris训练集前5行：'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mE:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_split\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseCrossValidator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_split\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_split\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGroupKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_split\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_split\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTimeSeriesSplit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mE:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msix\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwith_metaclass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_Iterable\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mIterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_pprint\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'comb'"
     ]
    }
   ],
   "source": [
    "#训练集与测试集分离阶段\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_df.iloc[:,0:4], \n",
    "            iris_df['target'], random_state= 14)\n",
    "print('Iris训练集前5行：')\n",
    "print(X_train.head(5))\n",
    "print('Iris训练集分类结果：')\n",
    "print(y_train.head(5).values)\n",
    "print('Iris测试集前5行：')\n",
    "print(X_test.head(5))\n",
    "print('Iris测试集原始分类结果')\n",
    "print(y_test.head(5).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#算法实施阶段\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()    #估计器，使用KNN算法\n",
    "knn.fit(X_train, y_train)\n",
    "y_predicted = knn.predict(X_test)\n",
    "print('Iris测试集真实结果')\n",
    "print(y_test.values)\n",
    "print('Iris测试集KNN算法预测结果')\n",
    "print(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#性能评估阶段\n",
    "accuracy = np.mean(y_predicted == y_test) *100 \n",
    "print('当前分类评估器是: knn')\n",
    "print('当前Accuracy是：%.1f' %accuracy + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#与多种算法比较\n",
    "from sklearn import tree, svm, naive_bayes,neighbors  \n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, \\\n",
    "             RandomForestClassifier, GradientBoostingClassifier #随机森林等算法\n",
    "\n",
    "clfs = {'svm': svm.SVC(gamma='auto'),\\\n",
    "        'decision_tree':tree.DecisionTreeClassifier(),\n",
    "        'naive_gaussian': naive_bayes.GaussianNB(), \\\n",
    "        'naive_mul':naive_bayes.MultinomialNB(),\\\n",
    "        'K_neighbor' : neighbors.KNeighborsClassifier(),\\\n",
    "        'bagging_knn' : BaggingClassifier(neighbors.KNeighborsClassifier(), max_samples=0.5,max_features=0.5), \\\n",
    "        'bagging_tree': BaggingClassifier(tree.DecisionTreeClassifier(), max_samples=0.5,max_features=0.5),\n",
    "        'random_forest' : RandomForestClassifier(n_estimators=50),\\\n",
    "        'adaboost':AdaBoostClassifier(n_estimators=50),\\\n",
    "        'gradient_boost' : GradientBoostingClassifier(n_estimators=50, learning_rate=1.0,max_depth=1, random_state=0)\n",
    "        }\n",
    "\n",
    "#用样本数据训练模型，用测试数据测试训练后的算法正确率\n",
    "def try_different_method(clf):\n",
    "    clf.fit(X_train,y_train)\n",
    "    score = clf.score(X_test,y_test)*100 #默认情况下分类估计器的score函数返回的是Accrracy\n",
    "    print('当前Accuracy是：%.1f'%score + '%')\n",
    "\n",
    "#依次调用clfs中的不同算法\n",
    "for clf_key in clfs.keys():\n",
    "    print('当前分类评估器是:',clf_key)\n",
    "    clf = clfs[clf_key]\n",
    "    try_different_method(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 10)\n",
      "加州房价数据前5行：\n",
      "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
      "0    -122.23     37.88                41.0        880.0           129.0   \n",
      "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
      "2    -122.24     37.85                52.0       1467.0           190.0   \n",
      "3    -122.25     37.85                52.0       1274.0           235.0   \n",
      "4    -122.25     37.85                52.0       1627.0           280.0   \n",
      "\n",
      "   population  households  median_income  median_house_value ocean_proximity  \n",
      "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
      "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
      "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
      "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
      "4       565.0       259.0         3.8462            342200.0        NEAR BAY  \n",
      "存在缺失值的数据前5行：\n",
      "     longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
      "290    -122.16     37.77                47.0       1256.0             NaN   \n",
      "341    -122.17     37.75                38.0        992.0             NaN   \n",
      "538    -122.28     37.78                29.0       5154.0             NaN   \n",
      "563    -122.24     37.75                45.0        891.0             NaN   \n",
      "696    -122.10     37.69                41.0        746.0             NaN   \n",
      "\n",
      "     population  households  median_income  median_house_value ocean_proximity  \n",
      "290       570.0       218.0         4.3750            161900.0        NEAR BAY  \n",
      "341       732.0       259.0         1.6196             85100.0        NEAR BAY  \n",
      "538      3741.0      1273.0         2.5762            173400.0        NEAR BAY  \n",
      "563       384.0       146.0         4.9489            247100.0        NEAR BAY  \n",
      "696       387.0       161.0         3.9063            178400.0        NEAR BAY  \n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_object_dtype_isnan'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0d514c3e7071>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[1;31m#使用SkLearn的Imputer处理缺失数据\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimpute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mimputer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"median\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#用该列中位数填充缺失值\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mimputer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcaDF\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mE:\\Anaconda3\\lib\\site-packages\\sklearn\\impute.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_object_dtype_isnan\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mis_scalar_nan\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_object_dtype_isnan'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#加利福利亚房价数据\n",
    "caDF = pd.read_csv('Section5 california_housing.csv')\n",
    "print(caDF.shape)\n",
    "print('加州房价数据前5行：')\n",
    "print(caDF.head())\n",
    "\n",
    "#观察数据，发现存在缺失值和非数值型字段\n",
    "sample_incompletedata = caDF[caDF.isnull().any(axis=1)]\n",
    "print('存在缺失值的数据前5行：')\n",
    "print(sample_incompletedata.head())\n",
    "\n",
    "#因为total_rooms和total_bedrooms线性相关性较强，\n",
    "#为了减少运算量，将total_bedrooms列舍去\n",
    "caDF.drop(['total_bedrooms'],axis=1,inplace=True)\n",
    "\n",
    "#ocean_proximity列是文本标签，描述了房屋离海滩距离\n",
    "#使用LabelEncoder将文本标签转换为数值标签\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "caDF[['ocean_proximity']]=caDF[['ocean_proximity']].apply\\\n",
    "                                          (LabelEncoder().fit_transform)\n",
    "\n",
    "#使用SkLearn的Imputer处理缺失数据\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\") #用该列中位数填充缺失值\n",
    "imputer.fit(caDF)\n",
    "#将所有数据转换为float类型\n",
    "caDF=pd.DataFrame(caDF,dtype=np.float)\n",
    "print('整理后数据前5行，请注意比原始数据少了一列：')\n",
    "print(caDF.head())\n",
    "\n",
    "#分离训练集与测试集，median_house_value列的数据是研究的目标\n",
    "from sklearn.model_selection import train_test_split\n",
    "caDFdata=caDF.drop(['median_house_value'],axis=1)\n",
    "caDFprice=caDF['median_house_value']\n",
    "Train_X,Test_X,Train_y,Test_y=train_test_split(caDFdata,caDFprice,\n",
    "                                              test_size=0.2,random_state=42)\n",
    "\n",
    "#数据标准化,将每一列数据转换为均值为0，方差为1的数据，便于后续处理\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss= StandardScaler().fit(Train_X)\n",
    "Train_X = pd.DataFrame(ss.transform(Train_X), columns=Train_X.columns)\n",
    "Test_X  = pd.DataFrame(ss.transform(Test_X),  columns=Test_X.columns)\n",
    "\n",
    "print('标准化后训练集前5行：')\n",
    "print(Train_X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#随机森林回归，是集成了多棵决策树而成的森林\n",
    "from sklearn import ensemble\n",
    "model_rf= ensemble.RandomForestRegressor(n_estimators=20)  # 使用20个决策树\n",
    "model_rf.fit(Train_X,Train_y)\n",
    "rf_score=model_rf.score(Test_X,Test_y)*100\n",
    "print('sklearn随机森林模型得分: %.1f' %rf_score + '%')\n",
    "rf_pred=model_rf.predict(Test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#人工神经网络回归，Sklearn中，又称多层感知机MLP\n",
    "from sklearn.neural_network import MLPRegressor  \n",
    "model_mlp = MLPRegressor(solver='lbfgs', hidden_layer_sizes=(5, 5),\n",
    "                         random_state=1)\n",
    "model_mlp.fit(Train_X,Train_y)\n",
    "mlp_score=model_mlp.score(Test_X,Test_y)*100\n",
    "print('sklearn人工神经网络回归模型得分: %.1f' %mlp_score + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#采用GridSearchCV来进行参数调整实验，找出最佳参数组合\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'solver':['lbfgs','sgd','adam'],\n",
    "              'hidden_layer_sizes': [(5,5),(10,10)]\n",
    "             }\n",
    "#对param_grid中的各参数进行组合，传递进MPL回归器。\n",
    "#cv=3,3折交叉验证，将数据集随机分为3份，每次将一份作为测试集，其他为训练集\n",
    "#n_jobs=-1，使用CPU核心数，-1表示所有可用的核\n",
    "best_mlp =GridSearchCV(MLPRegressor(max_iter=200),param_grid,cv=3,n_jobs=-1)\n",
    "best_mlp.fit(Train_X,Train_y)\n",
    "print('当前最佳参数组合：',best_mlp.best_params_)\n",
    "best_score=best_mlp.score(Test_X,Test_y)*100\n",
    "print('sklearn人工神经网络上述参数得分: %.1f' %best_score + '%')\n",
    "#用以上模型对Test_X进行预测\n",
    "mlp_pred = best_mlp.predict(Test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#绘图比较模型预测房价与真实房价的差异\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "plt.rcParams['font.sans-serif']=['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False \n",
    "fig = plt.figure(figsize=(10, 6))  # dpi参数指定绘图对象的分辨率，即每英寸多少个像素，缺省值为80\n",
    "#axes = fig.add_subplot(1, 1, 1)\n",
    "#为便于观察，只取部分结果作图\n",
    "T1=Test_y[:50]\n",
    "P1=mlp_pred[:50]\n",
    "R1=rf_pred[:50]\n",
    "plt.plot(range(len(T1)),T1, 'g',label='实际',linewidth=3)\n",
    "plt.plot(range(len(P1)),P1, 'y--',label='神经网络回归',linewidth=2)\n",
    "plt.plot(range(len(P1)),R1, 'r:',label='随机森林回归',linewidth=2)\n",
    "fig.tight_layout()\n",
    "plt.legend()\n",
    "plt.title('sklearn 回归模型')\n",
    "plt.savefig('skl_01.png',dpi=300,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
